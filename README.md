# LLM Questionnaires: Neural Anthropology for Generative AI

This repository contains the code and resources for our comprehensive study on intrinsic bias in Large Language Models (LLMs) and their capacity to represent diverse ideological and cultural perspectives through in-context prompting. We systematically evaluate the consistency with which LLMs respond to standardized psychological questionnaires under various minimal persona prompts, benchmarking their responses against robust human survey data. Our experimental design encompasses multiple state-of-the-art open-source models, with each model-persona pairing subjected to k repeated trials functioning as synthetic surveys. Our findings reveal significant variations in response consistency across different model architectures and highlight fundamental challenges in aligning LLMs with authentic human ideological positions through straightforward prompting techniques. This research contributes valuable insights for computational social scientists and AI ethicists employing LLMs as human simulacra in social science research, offering critical guidance on the limitations and potential methodological improvements for using language models in ideological representation tasks.