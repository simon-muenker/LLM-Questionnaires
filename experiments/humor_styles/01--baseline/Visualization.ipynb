{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import json\n",
    "\n",
    "import numpy\n",
    "import pandas \n",
    "\n",
    "import scipy\n",
    "import statsmodels\n",
    "import statsmodels.stats\n",
    "import statsmodels.stats.descriptivestats\n",
    "\n",
    "import seaborn\n",
    "\n",
    "import llm_questionnaires as llmq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS: typing.List[str] = llmq.CONSTANTS.MODELS\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS: typing.List[str] = [\n",
    "    str(quest[\"dimension\"][:2].upper()) + str(quest[\"dimension\"][4].upper()) + str(quest[\"id\"]) for quest in\n",
    "    json.load(open(\"../../../data/humor_styles/questionnaire.json\"))[0][\"questions\"]\n",
    "]\n",
    "COLUMNS[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw: typing.Dict[str, pandas.DataFrame]= {\n",
    "    **llmq.evaluation.read_populations(\"data/base\", MODELS, COLUMNS),\n",
    "    \"human_full\": (\n",
    "        pandas.read_csv(\"../../../data/humor_styles/survey.csv\")\n",
    "        .rename_axis(index=\"participant\")\n",
    "        .filter(like='Q', axis=1)\n",
    "        .replace(-1, None)\n",
    "        .apply(pandas.to_numeric)\n",
    "        .set_axis(COLUMNS, axis=1)\n",
    "    ),\n",
    "    \"random\": (\n",
    "        pandas.DataFrame(numpy.random.randint(0,5, (1000, 32)), columns=COLUMNS)\n",
    "        .rename_axis(index=\"participant\")\n",
    "    )\n",
    "}\n",
    "\n",
    "raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"llama3.1-8b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in raw.items():\n",
    "    values.to_csv(f\"reports/raw.{key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive = pandas.concat({\n",
    "    key: statsmodels.stats.descriptivestats.describe(values).T\n",
    "    for key, values in raw.items()\n",
    "})\n",
    "descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"human_sampled\"] = (\n",
    "    pandas.DataFrame(numpy.stack([\n",
    "        numpy.random.normal(row[\"mean\"], row[\"std\"], 1000).astype(int)\n",
    "        for _, row in descriptive.loc[\"human_full\"][[\"mean\", \"std\"]].iterrows()\n",
    "    ], axis=1), columns=range(1,33))\n",
    "    .rename_axis(index=\"participant\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.concat({\n",
    "    (key_1, key_2): (\n",
    "        pandas.Series(\n",
    "            scipy\n",
    "            .stats.f_oneway(\n",
    "                values_1,\n",
    "                values_2,\n",
    "                nan_policy=\"omit\"\n",
    "            )\n",
    "            .pvalue\n",
    "            > 0.05,\n",
    "            name=\"f_oneway_significant\"\n",
    "        )\n",
    "        .value_counts()\n",
    "    )\n",
    "    for key_1, values_1 in raw.items()\n",
    "    for key_2, values_2 in raw.items()\n",
    "}).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = (\n",
    "    llmq.evaluation.apply_calc_similarity(raw)\n",
    "    .to_frame()\n",
    "    .reset_index(level=[0, 1])\n",
    "    .pivot_table(values=\"similarity\", index=\"level_0\", columns=\"level_1\")\n",
    "    .reindex(\n",
    "        columns=(idx := [\"random\", \"human_sampled\", \"human_full\", *MODELS]),\n",
    "        index=idx\n",
    "    )\n",
    "    .pipe(llmq.evaluation.extract_triu_df)\n",
    ")\n",
    "sim\n",
    "\n",
    "print(sim.pipe(llmq.evaluation.format_latex_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.heatmap(sim, annot=True, mask=numpy.triu(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chunked = {\n",
    "    f\"{n:02d}\": chunk\n",
    "    for n, chunk in enumerate(numpy.array_split(raw[\"human_full\"].sample(frac=1.0), 10), start=1)\n",
    "}\n",
    "\n",
    "sim = (\n",
    "    llmq.evaluation.apply_calc_similarity(human_chunked)\n",
    "    .to_frame()\n",
    "    .reset_index(level=[0,1])\n",
    "    .pivot_table(values=\"similarity\", index=\"level_0\", columns=\"level_1\")\n",
    "    .pipe(llmq.evaluation.extract_triu_df)\n",
    ")\n",
    "sim\n",
    "\n",
    "print(sim.pipe(llmq.evaluation.format_latex_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
